---
title: multi-species occupancy from limited perceptability data
date: 2024
type: reference
tags: " #1_Scipt/community "
description: "Multi-species occupancy on detection space limited data"
---

**Script by ILT**


# Load packages
```R
library(tidyverse)
library(spOccupancy)
library(tidybayes)
library(boot)
#library(ggplot2)
#library(tidyr)
```

# Covariates & Occupancy dataframe
```R 
occupancy_data <- read.csv(file.path("0_Data/Processed/Limited perceptibility", "Occupancy_truncated_150.csv"))
covariates <- read_csv("0_Data/Processed/merged_covariates.csv")
dist_to_forest <- read_csv("0_Data/Processed/retn dist to nearest forest.csv")
colnames(covariates)
dist_to_forest <- select(dist_to_forest, location, Dist_near_forest)
covariates <- merge(covariates, dist_to_forest, by = "location")
covariates <- merge(covariates, occupancy_data[ ,c("location", "recording_date_time")])
View(covariates)
# covariates <- covariates %>% select(-c("TEWA", "OSFL", "RCKI", "YRWA", "REVI", "WTSP"))
covariates$DateTime <- covariates$recording_date_time
# Add amount edge
covariates$Edge <- ifelse(covariates$RETN_m2 != 0 & covariates$RETN_perimeter_m !=0, 
                            covariates$RETN_m2 / covariates$RETN_perimeter_m, 0)
covariates$Year_since_logging2 <- covariates$Year_since_logging ^2
# max(covariates$Edge) # 37.22
# Add patch presence
covariates <- covariates %>%
      mutate(Patch = ifelse(RETN_m2 > 0, 1, 0))

# This data has a few really large retention pathces. Let;s get rid of those
covariates <- covariates |>
  filter(RETN_m2 < 12000)
View(covariates)
# Scale some of these predictors
# predictors_to_scale <- c('RETN_m2', 'Year_since_logging', "Edge", "Year_since_logging2", "Dist_near_forest")

# Filter the sites in covariates so they match with fd_df
correct_sites <- occupancy_data$location
covariates_matching_sites <- covariates |>
                                filter(location %in% correct_sites)

nrow(covariates_matching_sites)
nrow(occupancy_data)

# n sites do not have any covariate data. These are likely becuase they didn't make the cut in terms of site quality.
# Exclude the sites that are not in covariate
sites_with_covs <- covariates_matching_sites$location
occupancy_data_correct_sites <- occupancy_data |>
                          filter(location %in% sites_with_covs)
nrow(occupancy_data_correct_sites) # 155 sites as of 2024-05-15. Down to 85 on 2024-04-17
nrow(covariates_matching_sites)
# write.csv(covariates_matching_sites, "0_Data/Processed/Limited perceptibility/Post truncation/covariates_prepped_comm.csv" )
# Now these two are mathcing in length (368 sites)
# Reset the names os it is clear
covariates <- covariates_matching_sites
occupancy_data <- occupancy_data_correct_sites

write.csv(covariates, "0_Data/Processed/Limited perceptibility/Post truncation/covariates_unscaled.csv")
write.csv(occupancy_data, "0_Data/Processed/Limited perceptibility/Post truncation/Multi_spp_occupancy.csv")

```



The species response is likely linked to their guild. I could add a categorical effect of the guild with which each speices is associated, which could be either the 
general habitat, so forest, cutblock, or generalist, or the nesting behaviour, like ground, shrub, and canopy.

I have two options, either I add it explicitely in the model, or I evaluate it afyer the model runs.

## What is the model?
https://doserlab.com/files/spoccupancy-web/
Multi-species occupancy modekl, which can be spatial to integrate landscape-level covariates. Models are fit using Pólya-Gamma data augmentation. Spatial models are fit using Gaussian processes.
Multiple single-species occupancy data sets using a joint likelihood framework. For multi-species models, spOccupancy provides functions to account for residual species correlations in a joint species distribution model framework while accounting for imperfect detection


# Run: multi species occupancy
```R 
# Load unscaled covariates data.
occupancy_data <- read.csv("0_Data/Processed/Limited perceptibility/Post truncation/Multi_spp_occupancy.csv")
covariates <- read.csv("0_Data/Processed/Limited perceptibility/Post truncation/covariates_unscaled.csv")
dim(occupancy_data)
# View(occupancy_data)
# For now, delete WIWR and WETA, since occu could not be calculated CHANGED, ALL GOOD
# occupancy_data <- occupancy_data %>%
#                             select(-c("WETA", "WIWR"))
# View(occupancy_data)
colnames(covariates)
# To test, let's select a handful of spp
# occupancy_data <- occupancy_data %>%
#                             select(c("location", 'recording_date_time',"WTSP", "SWTH", "LISP", "OVEN"))

# Select the columns of species detections
species_cols  <- names(occupancy_data)[1:61]
species_cols <- species_cols[! species_cols %in% c("SM2")]
# print(species_cols)

occupancy_format <- select(occupancy_data, location, species_cols)
# occupancy_format %>% select(all_of(species_cols))
# View(occupancy_format)

# Step 1: Convert data to long format where each row is a specific detection event
occupancy_long <- occupancy_format %>%
  pivot_longer(cols = species_cols, names_to = "species", values_to = "detection") %>%
  mutate(
    site_id = as.factor(location),  # Assuming 'location' is your site identifier column
    species_id = as.factor(species),
    replicate_id = 1  # Adjust if you have multiple visits per site
  )
# View(occupancy_long)

# Step 2: Create an array & fill with species, sites, replicates
# Determine dimensions
n_species <- length(unique(occupancy_long$species_id))
n_sites <- length(unique(occupancy_long$site_id))
n_replicates <- max(occupancy_long$replicate_id)  # Should be 1 

# Create array
y_array <- array(dim = c(n_species, n_sites, n_replicates))

# Fill array
for (i in 1:n_species) {
  for (j in 1:n_sites) {
    # Subsetting to match species and site
    data_subset <- occupancy_long %>%
      filter(species_id == levels(occupancy_long$species_id)[i],
             site_id == levels(occupancy_long$site_id)[j])
    
    y_array[i, j, 1:length(data_subset$replicate_id)] <- data_subset$detection
  }
}

# Verify structure
dim(y_array) # Num of species * num of sites * num of replicates (visits) # 05/13: 61*155*1 # 04/23: 62*155*1
# View(covariates)
# Prepare `occ.covs`: covariate for occurrence
# occ_covs <- as.matrix(covariates$Year_since_logging) # get 1 column
occ_covs <- cbind(covariates$Year_since_logging, covariates$Patch, covariates$RETN_m2, covariates$Edge)
dim(occ_covs) # number of sites
# print(occ_covs)
# scaled_occ_covs <- scale(occ_covs)
# occ_covs <- scaled_occ_covs
colnames(occ_covs) <- c("Year_since_logging", "Patch", "Area", "Edge")
colnames(occ_covs) # sites * covariates

# Prepare detection covariates. This one is a list of matrices.
covariates$doy <- yday(covariates$DateTime)
covariates <- covariates %>%
  mutate(minutes = hour(DateTime) * 60 + minute(DateTime))
covariates$tod <- covariates$minutes

# The format of the detection data is a list of the covariates, where each covariate has a name and is a matrix object 
# with the number of visits as the columns.

det_covs <- list(doy = as.matrix(covariates$doy), tod = as.matrix(covariates$tod))
# colnames(det_covs) # <- "V1" # variable 1
# View(det_covs$doy)
str(occ_covs)
head(occ_covs)
# Package all the data into a list for msPGOcc
data_list <- list(y = y_array, occ.covs = occ_covs, det.covs = det_covs)
dim(occ_covs)
# Check for NA, NaN, or Inf values in your dataset
sum(is.na(data_list))  # should be 0

# Calculate correct dimensions
# n_occ_covs <- ncol(occ_covs)  # Number of occupancy covariates
# n_det_covs <- length(det_covs)  # Number of detection covariates

# Initial values
inits.list <- list(alpha.comm = 0, 
                   beta.comm = 0, 
                   beta = 0, 
                   alpha = 0,
                   tau.sq.beta = 1, 
                   tau.sq.alpha = 1, 
                   z = apply(y_array, c(1, 2), max, na.rm = TRUE))
str(inits.list)
# Priors list
prior.list <- list(beta.comm.normal = list(mean = 0, var = 2.72), 
                   alpha.comm.normal = list(mean = 0, var = 2.72), 
                   tau.sq.beta.ig = list(a = 0.1, b = 0.1), 
                   tau.sq.alpha.ig = list(a = 0.1, b = 0.1))

# Define the number of samples to draw
n_samples <- 3000
n_burn <- 1000
n_thin <- 1

# # Can also do this for the formulas:
# occ.ms.formula <- ~ scale(Year_since_logging) + scale(I(Year_since_logging^2)) + Patch
occ.ms.formula <- ~ scale(Year_since_logging)*Patch + scale(poly(Year_since_logging^2)) + scale(Area)
det.ms.formula <- ~ scale(doy) + scale(tod)
# Fit the multi-species occupancy model
model_ms_inter <- msPGOcc(
  # occ.formula = ~ scale(Year_since_logging) + scale(I(Year_since_logging^2)),         # lme4 formula for occupancy covariates
  # det.formula = ~ scale(doy),                        # lme4 formula for detection covariates
  occ.formula = occ.ms.formula,
  det.formula = det.ms.formula,
  data = data_list,                           # List containing the data
  inits = inits.list,                         # Initial values
  priors = prior.list,                        # Priors for the parameters
  n.samples = n_samples,                      # Number of samples to collect in each chain
  n.burn = n_burn,                            # Number of burn-in samples
  n.thin = n_thin,                            # Thinning interval
  n.chains = 1,                               # Number of chains to run
  verbose = TRUE                              # Print progress to the screen
)

# Print the summary of the model
summary(model_ms_2, level = 'both')
summary(model_ms_inter, level = 'community')

# Posterior predictivce check
# Approx. run time: 20 sec
ppc.ms.out <- ppcOcc(model_ms, 'chi-squared', group = 1) # Goup = 1 is across sites
# ppc.model <- ppcOcc(model, fit.stat = 'freeman-tukey', group = 1) # Goup = 1 is across sites
summary(ppc.ms.out)
waicOcc(model)

waicOcc(model, by.sp = TRUE)
```

# Model evaluation
```R 
# Print the summary of the model
# In the Doser example, there are no rhats either
# summary(model, level = 'both')
summary(model_ms, level = 'community')

# Posterior predictivce check
# Approx. run time: 20 sec
ppc.ms.out <- ppcOcc(model_ms, 'chi-squared', group = 1) # Goup = 1 is across sites
# ppc.model <- ppcOcc(model, fit.stat = 'freeman-tukey', group = 1) # Goup = 1 is across sites
summary(ppc.ms.out)
waicOcc(model)

waicOcc(model_ms, by.sp = TRUE)

## There are some species WAIC that are infinite. Identify those species and find out more
occupancy_data <- read.csv("0_Data/Processed/Limited perceptibility/Multi_spp_occupancy.csv")
names(occupancy_data[47])
# WETA, WIWR: infinite WAIC.

```

# Plot posteriors, one species at a time
```R 
library(tidybayes)
# Density of covariates
library(coda)
library(boot)
# densplot(model_ms$beta.samples) have to find a better way to plot all these posterior densities

# let's test one species

# Get the intercept posteriors 
# model_ms$beta.samples is a coda object of posteriors
dim(model_ms_inter$beta.samples) # draws * length of predictors for each species
summary(model_ms_inter$beta.samples)
print(species_cols[33])
beta_post <- as.data.frame(as.matrix(model_ms_inter$beta.samples))
View(beta_post)
colnames(beta_post)
head(beta_post[1])
intercept_psi_OCWA <- mean(beta_post$"(Intercept)-sp33")
slope_harvage <- mean(beta_post$"scale(Year_since_logging)-sp33")
lower_harvage <- unname(quantile(x = beta_post$"scale(Year_since_logging)-sp33", probs = 0.025))
upper_harvage <- unname(quantile(x = beta_post$"scale(Year_since_logging)-sp33", probs = 0.975))

slope_harvage2 <- mean(beta_post$"scale(I(Year_since_logging^2))-sp33")
lower_harvage2 <- unname(quantile(x = beta_post$"scale(I(Year_since_logging^2))-sp33", probs = 0.025))
upper_harvage2 <- unname(quantile(x = beta_post$"scale(I(Year_since_logging^2))-sp33", probs = 0.975))

mean_pred_harvage <- inv.logit(intercept_psi_ALFL + slope_harvage * seq(1,22, length.out = 100) + slope_harvage2* seq(1,22, length.out = 100))
lower_pred_harvage <- inv.logit(intercept_psi_ALFL + lower_harvage * seq(1,22, length.out = 100) + lower_harvage2* seq(1,22, length.out = 100))
upper_pred_harvage <- inv.logit(intercept_psi_ALFL + upper_harvage * seq(1,22, length.out = 100) + upper_harvage2* seq(1,22, length.out = 100))
scale(seq(1,22, length.out = 100))

# Data frame for plotting
plot_data <- data.frame(
  Harv_age = seq(1,22, length.out = 100),
  Psi = mean_pred_harvage,
  LowerCI = lower_pred_harvage,
  UpperCI = upper_pred_harvage
)

# Plotting with ggplot2
ggplot(plot_data, aes(x = Harv_age, y = Psi)) +
  geom_ribbon(aes(ymin = LowerCI, ymax = UpperCI), fill = "darkgreen", alpha = 0.1) +
  geom_line(color = "darkgreen") +
  xlab("Harvest Age") +
  ylab("Estimated Probability of initial occupancy (Psi)") +
  ggtitle("OCWA") +
  theme_classic() +
  theme(
    text = element_text(family = "Times New Roman", size = 14),
    axis.title = element_text(color = "black")
  )


```

# Posterior plots: harvest age combined

```R 
# Get an array of the species id in the model output
species_ids <- c()  # or simply use species_array <- c()
# str(species_ids)
# Loop to append species identifiers
# Change eventually so it looks for al lthe 4-letter code, not manually magic numbers
for (i in 1:61) {
  species <- paste0("sp", i)
  species_ids <- c(species_ids, species)
}

# Check the result
print(species_ids)

species_names <- species_cols[! species_cols %in% c("SM2")]
# Prepare a dataframe to store all plotting data
all_plot_data <- data.frame()

# Loop through each species
for (i in seq_along(species_ids)) {
    species_id <- species_ids[i]
    species_name <- species_names[i]
    
    # occupancy lofit formula: scale(Year_since_logging)*Patch + scale(poly(Year_since_logging^2)) + scale(Area)
    # Extract coefficients for this species
    intercept_psi <- mean(beta_post[[paste("(Intercept)-", species_id, sep="")]])

    slope_harvage <- mean(beta_post[[paste("scale(Year_since_logging)-", species_id, sep="")]])
    lower_harvage <- unname(quantile(beta_post[[paste("scale(Year_since_logging)-", species_id, sep="")]], probs = 0.025))
    upper_harvage <- unname(quantile(beta_post[[paste("scale(Year_since_logging)-", species_id, sep="")]], probs = 0.975))
    
    slope_harvage2 <- mean(beta_post[[paste("scale(poly(Year_since_logging^2))-", species_id, sep="")]])
    lower_harvage2 <- unname(quantile(beta_post[[paste("scale(poly(Year_since_logging^2))-", species_id, sep="")]], probs = 0.025))
    upper_harvage2 <- unname(quantile(beta_post[[paste("scale(poly(Year_since_logging^2))-", species_id, sep="")]], probs = 0.975))
    
    slope_patch <-mean(beta_post[[paste("Patch-", species_id, sep="")]])
    lower_patch <- unname(quantile(beta_post[[paste("Patch-", species_id, sep="")]], probs = 0.025))
    upper_patch <- unname(quantile(beta_post[[paste("Patch-", species_id, sep="")]], probs = 0.975))

    slope_area <-mean(beta_post[[paste("scale(Area)-", species_id, sep="")]])
    lower_area <- unname(quantile(beta_post[[paste("scale(Area)-", species_id, sep="")]], probs = 0.025))
    upper_area <- unname(quantile(beta_post[[paste("scale(Area)-", species_id, sep="")]], probs = 0.975))

    # Generate predictions
    Harv_age <- scale(seq(1, 22, length.out = 100))
    # all harv age vary
    # mean_pred_harvage <- inv.logit(intercept_psi + slope_harvage * Harv_age + slope_harvage2 * Harv_age^2)
    # lower_pred_harvage <- inv.logit(intercept_psi + lower_harvage * Harv_age + lower_harvage2 * Harv_age^2)
    # upper_pred_harvage <- inv.logit(intercept_psi + upper_harvage * Harv_age + upper_harvage2 * Harv_age^2)

      mean_pred_harvage <- inv.logit(intercept_psi + slope_harvage * Harv_age)
    lower_pred_harvage <- inv.logit(intercept_psi + lower_harvage * Harv_age)
    upper_pred_harvage <- inv.logit(intercept_psi + upper_harvage * Harv_age)

    # # # no variation form harvage^2
    # # mean_pred_harvage <- inv.logit(intercept_psi + slope_harvage * Harv_age)
    # # lower_pred_harvage <- inv.logit(intercept_psi + lower_harvage * Harv_age)
    # # upper_pred_harvage <- inv.logit(intercept_psi + upper_harvage * Harv_age)

    # Create a dataframe for this species
    species_plot_data <- data.frame(
      Harv_age = Harv_age,
      Psi = mean_pred_harvage,
      LowerCI = lower_pred_harvage,
      UpperCI = upper_pred_harvage,
      Species = species_name
    )

    # Append to the main dataframe
    all_plot_data <- rbind(all_plot_data, species_plot_data)
}

# Convert Species to a factor for proper plotting
all_plot_data$Species <- factor(all_plot_data$Species, levels = species_names)

# Plotting all species in one figure with facets
p <- ggplot(all_plot_data, aes(x = Harv_age, y = Psi)) +
    geom_ribbon(aes(ymin = LowerCI, ymax = UpperCI), fill = "darkgreen", alpha = 0.1) +
    geom_line(color = "darkgreen") +
    facet_wrap(~ Species, scales = "free_y") +
    labs(x = "Harvest Age", y = "Estimated Probability of Occupancy (Psi)") +
    theme_classic() +
    theme(text = element_text(size = 14),
          axis.title = element_text(color = "black"))

# Print the plot
# Something is wrong with this plot, I think. 2024/04/23
print(p)

ggsave(file.path("2_Outputs/Limited perceptibility/species_plots_occupancy/Harvest Age", "PsiHarvAge95CI_age.png"), p, width = 20, height = 15)


```
# What is multi-species occupancy modeling?
Multi-species occupancy is use to infer partially observed occupancy status, ${z_i}$, from the detection probability and the assumption that the observed occurrence is smaller than the actual occurrence.
From a repeat visits persepective, the species that should be in the area and were never actually observed have an estimated occupancy status.
On one hand, the multi-species occupancy model is used to parameterise detectio nand occupancy probabilities using our covariates of interest, such as age of harvest or time of day, on both of those estimated quantites.
On the other, we can use the inferred occupancy status of all the species to derive ecologically interesting estimands, such as actual species richness (requires repeat visit).

```R 
z <- 0
0.1^z

```

# Plot occupancy probability
```R 
# Community
# Psi


# 1. getting to know the MCMC output----
# model_ms$psi.samples: 3-dim array of posterior samples for occupancy prob for each species
length(model_ms$psi.samples) #19220000
model_ms$psi.samples[1,1,1]
str(model_ms$psi.samples) #  list of 3: species * site * iterations
# Why is it 3 dimensions?
dim(model_ms$psi.samples) # draws * species * sites
as_tibble(model_ms$psi.samples) %>%
  slice(1)

# how many draws?
length(model_ms$psi.samples[,1,1]) #2000 draws, also n_samples - n_burn
n_draws <- n_samples - n_burn
m1 <- mean(model_ms$psi.samples[, 1,1]) # This is the mean occ prob for ALFL at site 1
# model_ms$psi.samples[, 1,1] # each site has a posterior of occupancy probability of density n draws
median(model_ms$psi.samples[, 1,1])
q1 <- unname(quantile(x = model_ms$psi.samples[, 1,1], probs = 0.025)) 


species_cols
for (species in 1:length(species_cols)) {
  print(species)
  mean_psi <- model_ms$psi.samples[species,1,1]
}


# 2. Plot here---
# Number of species
num_species <- 61
Year_since_logging <- covariates$Year_since_logging
Area <- covariates$RETN_m2
Edge <- covariates$Edge
Patch <- covariates$Patch
# Initialize list to store data frames
species_dfs <- vector("list", num_species)
# Loop through each species
for (species in 1:num_species) {
    # Extract the data for this species across all sites
    species_data <- model_ms$psi.samples[, species, ]

    # Initialize a matrix to store the results for this species
    species_results <- matrix(nrow = 85, ncol = 8)  # 85 sites, 4 statistics + covariate
    colnames(species_results) <- c("Year_since_logging", "Area", "Edge", "Patch", "Mean", "Median", "Q0.25", "Q97.5")

    # Loop through each site
    for (site in 1:85) {
        # Extract data for this site
        site_data <- species_data[, site]
        species_results[site, "Year_since_logging"] <- Year_since_logging[site]
        species_results[site, "Area"] <- Area[site]
        species_results[site, "Edge"] <- Edge[site]
        species_results[site, "Patch"] <- Patch[site]
        # Calculate statistics
        species_results[site, "Mean"] <- mean(site_data)
        species_results[site, "Median"] <- median(site_data)
        species_results[site, "Q0.25"] <- quantile(site_data, probs = 0.025) # Credible intervla for psi, not the covariate
        species_results[site, "Q97.5"] <- quantile(site_data, probs = 0.975)
    }

    # Convert the matrix to a dataframe and store in the list
    species_dfs[[species]] <- as.data.frame(species_results)
}

# 
print(species_cols[12])
# Check with one species
ggplot(species_dfs[[12]], aes(x = Year_since_logging, y = Mean)) +
    geom_line() +
    labs(title = "Mean Psi Across year since logging for BTNW", x = "age", y = "Mean Psi")


# Create a directory to save plots, if it does not already exist
plot_dir <- "species_plots_occupancy/area"
if (!dir.exists(file.path("2_Outputs/Limited perceptibility", plot_dir))) {
  dir.create(file.path("2_Outputs/Limited perceptibility", plot_dir))
}

# One plot per species
# Loop through each species and generate a plot
for (species_index in 1:num_species) {
  # Create the plot
  p <- ggplot(species_dfs[[species_index]], aes(x = Year_since_logging, y = Mean)) +
    geom_line(color = "blue") +
    geom_point(color = "red") +
    labs(title = species_cols[species_index],
         x = "Years Since Logging", y = "Mean Psi") +
    theme_minimal()
  
  # Display plot in RStudio Viewer or R GUI
  print(p)

  # Save the plot to a file
  # ggsave(filename = paste0("2_Outputs/Limited perceptibility/species_plots_occupancy/Patch",  "/", species_cols[species_index], "_psi_plot.png"), plot = p, width = 8, height = 6)
}

# All plots into 1 figure
# Combine all data frames into one, adding a 'Species' column
combined_df <- do.call(rbind, lapply(1:num_species, function(i) {
  cbind(species_dfs[[i]], Species = species_cols[i])
}))

# Convert 'Species' to a factor for better handling in ggplot2
combined_df$Species <- factor(combined_df$Species)
# Create a single plot with facets for each species
p <- ggplot(combined_df, aes(x = Year_since_logging, y = Mean)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = Q0.25, ymax = Q97.5), fill = "blue", alpha = 0.2) +  # ribbon for CI
  # geom_point(color = "red") +
  facet_wrap(~ Species, scales = "free_y") +  # 'free_y vs fixed_y
  labs(x = "Harvest age", y = "Mean Psi") +
  theme_minimal() +
  theme(strip.text.x = element_text(size = 8),  
        panel.spacing = unit(1, "lines"))  

# Display the plot
print(p)

# Optionally save the plot to a file
ggsave(file.path("2_Outputs/Limited perceptibility/species_plots_occupancy/Area", "combined_psi_freey_plots_median_95CI.png"), p, width = 20, height = 15)  


length(model_ms$psi.samples[, 1,1])
AFLF_s1_draws <- model_ms$psi_samples[1:n_draws, 1,1]
View(model_ms$psi.samples[, 1,1])
explore_array <- model_ms$psi.samples
explore_array[1,1,] # 85 length, This is the site level
explore_array[1,,] # This is the species * site
explore_array[,,1] # This is all the draws
summary(explore_array)

```

# Plot posterior densities
Each species has a posterior estiamte for each covariate. What is the best graphical representation?
```R 
dim(model_ms)
densplot(model_ms$beta.comm.samples)

# Let's make prettier violinplots of the posterior estimates, for the community first
comm_post <- as.data.frame(model_ms$beta.comm.samples)
comm_post_long <- comm_post %>%
  pivot_longer(cols = everything(), names_to = "Parameter", values_to = "Estimate")

p <- ggplot(comm_post_long, aes(x = Parameter, y = Estimate, fill = Parameter)) +
  geom_violin(trim = FALSE) +
  theme_minimal() +
  labs(
       x = "Parameter",
       y = "Community Posteriors") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(legend.position = "none")
p
ggsave(filename = "2_Outputs/Limited perceptibility/Posterior plots/Community_harv_age.png", plot = p, width = 4, height = 3)
# Violinplots of the 

```

# Species richness
From a repeat visit framework, the inferred z of unobserved or rare species can be estimated from the detection submodel. These can be added to the observed speices richness to get more accurate estiamte.
```R 


```

# Spatial multi-species occupancy model 

```R 
# Load unscaled covariates data.
occupancy_data <- read.csv("0_Data/Processed/Limited perceptibility/Post truncation/Multi_spp_occupancy.csv")
covariates <- read.csv("0_Data/Processed/Limited perceptibility/Post truncation/covariates_unscaled.csv")

colnames(occupancy_data)
species_cols  <- names(occupancy_data)[1:62]

# Step 1: Convert data to long format where each row is a specific detection event
occupancy_format <- select(occupancy_data, location, species_cols)

occupancy_long <- occupancy_format %>%
  pivot_longer(cols = species_cols, names_to = "species", values_to = "detection") %>%
  mutate(
    site_id = as.factor(location),  # Assuming 'location' is your site identifier column
    species_id = as.factor(species),
    replicate_id = 1  # Adjust if you have multiple visits per site
  )
# View(occupancy_long)

# Step 2: Create an array & fill with species, sites, replicates
# Determine dimensions
n_species <- length(unique(occupancy_long$species_id))
n_sites <- length(unique(occupancy_long$site_id))
n_replicates <- max(occupancy_long$replicate_id)  # Should be 1 

# array
y_array <- array(dim = c(n_species, n_sites, n_replicates))

# Fill array
for (i in 1:n_species) {
  for (j in 1:n_sites) {
    # Subsetting to match species and site
    data_subset <- occupancy_long %>%
      filter(species_id == levels(occupancy_long$species_id)[i],
             site_id == levels(occupancy_long$site_id)[j])
    
    y_array[i, j, 1:length(data_subset$replicate_id)] <- data_subset$detection
  }
}

occ_covs <- cbind(covariates$Year_since_logging, covariates$Patch)
is.matrix(occ_covs) # good
colnames(occ_covs) <- c("Year_since_logging", "Patch")
covariates$doy <- yday(occupancy_data$recording_date_time)
occupancy_data <- occupancy_data %>%
  mutate(minutes = hour(recording_date_time) * 60 + minute(recording_date_time))
covariates$tod <- occupancy_data$minutes

det_covs <- list(doy = as.matrix(covariates$doy), tod = as.matrix(covariates$tod))
is.matrix(det_covs$doy)
is.matrix(det_covs$tod)
# coords is a J×2 matrix of the observation coordinates.
# Note that spOccupancy assumes coordinates are specified in a projected coordinate system.
coords <- cbind(covariates$latitude, covariates$longitude)
is.matrix(coords) # good
data_list <- list(y = y_array, occ.covs = occ_covs, det.covs = det_covs, coords = coords)

# Initial values, priors, and batches are form the example here (https://www.jeffdoser.com/files/spoccupancy-web/reference/spmspgocc)
# Number of batches
n.batch <- 30
# Batch length
batch.length <- 25
n.samples <- n.batch * batch.length

# Priors
prior.list <- list(beta.comm.normal = list(mean = 0, var = 2.72), 
                   alpha.comm.normal = list(mean = 0, var = 2.72), 
                   tau.sq.beta.ig = list(a = 0.1, b = 0.1), 
                   tau.sq.alpha.ig = list(a = 0.1, b = 0.1),
                   phi.unif = list(a = 3/1, b = 3/.1), 
                   sigma.sq.ig = list(a = 2, b = 2)) 
# Initial values
inits.list <- list(alpha.comm = 0, 
                   beta.comm = 0, 
                   beta = 0, 
                   alpha = 0,
                   tau.sq.beta = 1, 
                   tau.sq.alpha = 1, 
                   phi = 3 / .5, 
                   sigma.sq = 2,
                  #  w = matrix(0, nrow = N, ncol = nrow(X)), # I don't know what this is
                   z = apply(y_array, c(1, 2), max, na.rm = TRUE)) # updated with my objects
# Tuning
tuning.list <- list(phi = 1) 

occ.ms.formula <- ~ scale(Year_since_logging)*Patch
det.ms.formula <- ~ scale(doy) + scale(tod)

sp_ms_Occ <- spMsPGOcc(occ.formula = occ.ms.formula, 
                 det.formula = det.ms.formula, 
                 data = data_list,
                 inits = inits.list, 
                 n.batch = n.batch, # see Roberts and Rosenthal (2009)
                 batch.length = batch.length, # see Roberts and Rosenthal (2009)
                 accept.rate = 0.43, # this is deafult. See Roberts and Rosenthal (2009)
                 priors = prior.list, 
                 cov.model = "exponential", 
                 tuning = tuning.list, 
                 n.omp.threads = 1, 
                 verbose = TRUE, 
                 NNGP = TRUE, # this sets it to Nearest Neighbour, instead of Gaussian Process
                 n.neighbors = 5, # from the paper on this topic (Datta et al., 2016), says 15 is good, but 5 might even be ok
                 search.type = 'cb', # nearest neighbour search algorithm. 'cb' is fastest
                 n.report = 10, # dont know
                 n.burn = 500, 
                 n.thin = 1, 
                 n.chains = 1)

summary(sp_ms_Occ, level = 'community')


```
